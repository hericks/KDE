---
title: "Introduction to kernel density estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to KDE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

In this package we are trying to answer the following question: given $n$ (numeric) samples,
which are distributed according to a cumulative distribution function $F$ and probability density function $f$ (with respect to the Lebesgue measure), how can we estimate the density $f$?

Kernel density estimation is the chosen method that should provide us with a solution for this problem. Our estimator for the desired density $f$ with samples $x_1,...,x_n$ will be the function
\begin{align}
\hat{f_h}(x) = \frac{1}{hn} \sum\limits^{n}_{i=0} K(\frac{x-x_i}{h})
\end{align} 
which is called the *kernel density estimator* (KDE).
It depends on a real number $\mathcal{h}$, the so called *bandwidth* and an integrable function $K \colon \mathbb{R} \rightarrow \mathbb{R}$ such 
that $\int\limits^{\infty}_{-\infty}K(x) \ dx=1$, which is called *kernel*. \
The package is equipped with the function `kernel_density_estimator`, which is an implementation of the function described above. 

With the three bandwidth estimation functions `cross_validation`, `goldenshluger_lepski` and `pco_method` the package focuses 
on solving the problem by estimating the optimal bandwidth for a given kernel.


## Getting started
Load the package with: 
```{r setup}
library(KDE)
```

## Base structure

In order to secure consistency and fulfillment of the requirements for each element used in solving the above problem with a KDE, this package works on S3 classes for densities and kernels.
Both classes inherit from the class `IntegrableFunction` (for more information see `?IntegrableFucntion`).
First of all, we need to construct our objects.

### Density
The density function we want to sample from is a sine curve on the interval $[0,1]$. If samples are already given, the reader might skip this part.
```{r}
# Custom density
f_dens <- function(x) {
  ret <- 1 + sin(2*pi*x)
  ret[x < 0 | 1 < x] <- 0
  ret
}
support_density <- c(0,1)
# constructing the Density object
dens <- Density(f_dens, support_density, subdivisions=1000L)
print(dens)
```
Now we can see that the density is an object of the class `Density`. In addition to the function, the object is equipped with a support and subdivisions parameter. Both are needed for the `stats::integrate` function to work properly.
To see the requirements for constructing a `Density` object use `?Density`.

### Kernel
This package gives you the possibility to build your own custom kernels:
```{r}
# build a Kernel object yourself: this is the sigmoid function
f_ker <- function(x){
  return(2/pi * (1/(exp(x) + exp(-x))))
}
support_sigmoid <- c(-20,20)
# constructing a Kernel object
sigmoid_kernel <- Kernel(f_ker, support_sigmoid, subdivisions=1000L)
print(sigmoid_kernel)
```
Note that in the case of the support being `c(-Inf,Inf)` a kernel function is sufficiently small outside of a compact interval, such that the numeric integration outside of it will be zero.
We need to choose a finite interval as support for some bandwidth estimators to work properly.

To see the requirements for constructing a Kernel object use `?Kernel`.

### Builtin Kernels
This package also provides a selection of various built-in kernels:
```{r}
# using a built-in Kernel
gaussian_kernel <- gaussian
print(gaussian_kernel)
```
Run `?Kernel` to see the full list of the built-in kernels.

### Sampling 
In case you don't already have a set of samples, you can use a built-in rejection sampling method to create a set of samples from Density objects: 
```{r}
# Create sampler from custom density
g_den <- Density(dunif, c(0,1))
custom_sampler <- rejection_sampling(dens, g_den, runif, 2)
# Sample from custom sampler
num_samples <- 100
samples <- custom_sampler(num_samples)
```
Now that we have chosen our kernel and a set of samples, it is time to choose a suitable bandwidth to finally bild the KDE.

## Bandwidth Estimation

The general idea behind all following bandwidth estimation methods is that the optimal bandwidth $\mathcal{h}$ is the one that minimizes the maximal risk 
\begin{align}
\mathbb{E}(||\hat{f_h}-f||^2)
\end{align} where $\hat{f_h}$ is the KDE with bandwidth $\mathcal{h}$ and $f$ the density we like to estimate. Note that to realize this idea, each method chooses the optimal bandwidth from a given finite collection $\mathcal{H}_n$. \
Now the problem lies in the fact that $f$ is unknown, so we can't calculate the risk directly, therefore this package provides three different approaches to solve that and to estimate the most suitable bandwidth from a given collection $\mathcal{H}_n$.

### Logarithmic Bandwidth Set

In case you don't already have a bandwidth collection to work with, you can use the function `logarithmic_bandwidth_set` which provides you with an admissible bandwidth set in a given range.  
```{r}
# setting up bandwidth sets
bandwidth_set <- logarithmic_bandwidth_set(from=1/length(samples), to=1, length.out=10)
```
Note that for the bandwidth estimation methods `length.out` has to be less than or equal to `length(samples)`.


### Cross Validation
The first method to estimate a bandwidth is called *cross-validation* or *leave-one-out*. For more information run `?cross_validation`.
```{r}
# cross validation method
cv_gaussian <- cross_validation(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
cv_sigmoid <- cross_validation(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=500L)
cat("bandwidth for gaussian kernel: ", cv_gaussian, "\nbandwidth for sigmoid kernel: ", cv_sigmoid)
```

### Goldenshluger-Lepski Method
The second approach is a implementation of the *Goldenshluger-Lepski method*. To understand the function in detail see `?goldenshluger_lepski`.
```{r}
# goldenshluger-lepski method
gl_gaussian <- goldenshluger_lepski(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
gl_sigmoid <- goldenshluger_lepski(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=200L)
cat("bandwidth for gaussian kernel: ", gl_gaussian, "\nbandwidth for sigmoid kernel: ", gl_sigmoid)
```

### Penalized Comparison to Overfitting
The third method for estimating a bandwidth is called *Penalized Comparison to Overfitting (PCO)*. If you want to look up further details about the function `pco_method` run `?pco_method`.
```{r}
# Penalized Comparison to Overfitting
pco_gaussian <- pco_method(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
pco_sigmoid <- pco_method(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=250L)
cat("bandwidth for gaussian kernel: ", pco_gaussian, "\nbandwidth for sigmoid kernel: ", pco_sigmoid)
```


## Kernel-Density-Estimation
Now that we have determined some suggestions for our bandwidth $h$ we can finally estimate our density function $f$ using the kernel density estimator.

### Calculating the KDE function
```{r}
kde_cv_gaussian <- kernel_density_estimator(gaussian_kernel, samples, cv_gaussian, subdivisions=100L)
kde_gl_gaussian <- kernel_density_estimator(gaussian_kernel, samples, gl_gaussian, subdivisions=100L)
kde_pco_gaussian <- kernel_density_estimator(gaussian_kernel, samples, pco_gaussian, subdivisions=100L)

kde_cv_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, cv_sigmoid, subdivisions=200L)
kde_gl_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, gl_sigmoid, subdivisions=200L)
kde_pco_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, pco_sigmoid, subdivisions=200L)
```
Note that the  `kernel_density_estimator` returns a `IntegrableFunction` object. 

## Kernel and bandwidth comparison 
Keep in mind that two different bandwidth estimators can choose the same bandwidth and therefore, their KDE will be the same. For a small comparison of our calculated estimators, we will use the following ISE function:
```{r}
# comparing the ISE
ISE <- function(kde, dens, subdivisions=100L){
  support <- range(kde$support, dens$support)
  integrate(function(x){(kde$fun(x)-dens$fun(x))^2}, 
                   lower=support[1], upper=support[2], 
                   subdivisions=subdivisions)$value
}
```

```{r,echo=FALSE} 

# ISE for the gaussian kernel
cat("ISE for cross validation on the gaussian kernel: ", ISE(kde_cv_gaussian, dens))
cat("ISE for Goldenshluger-Lepski method on the gaussian kernel: ", ISE(kde_gl_gaussian, dens))
cat("ISE for PCO method on the gaussian kernel: ", ISE(kde_pco_gaussian, dens))

```
```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
# plotting KDE and functionx_lim
x_lim_lower <- -0.5
x_lim_upper<- 1.5
x <- seq(from = x_lim_lower, to = x_lim_upper, length.out=1000)
plot(x, dens$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the gaussian kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2

)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_gaussian$fun(x), col = "violet")
lines(x,
      kde_gl_gaussian$fun(x), col = "steelblue2")
lines(x,
      kde_pco_gaussian$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")




```

```{r,echo=FALSE} 
# ISE for the sigmoid kernel
cat("ISE for cross validation method on the sigmoid kernel: ", ISE(kde_cv_sigmoid, dens))
cat("ISE for Goldenshluger-Lepski method method on the sigmoid kernel: ", ISE(kde_gl_sigmoid, dens))
cat("ISE for PCO method method on the sigmoid kernel: ", ISE(kde_pco_sigmoid, dens))

```

```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
plot(x, dens$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the sigmoid kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2
)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_sigmoid$fun(x), col = "violet")
lines(x,
      kde_gl_sigmoid$fun(x), col = "steelblue2")
lines(x,
      kde_pco_sigmoid$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")

```

At last, a side by side comparison of the above calculated ISE's:
```{r, echo=FALSE}
table <- data.frame(method=c("cross_validation ISE", "goldenshluger_lepski ISE", "pco_method ISE"),
                    gaussian=c(ISE(kde_cv_gaussian, dens), ISE(kde_gl_gaussian, dens), ISE(kde_pco_gaussian, dens)),
                    sigmoid=c(ISE(kde_cv_sigmoid, dens), ISE(kde_gl_sigmoid, dens),  ISE(kde_pco_sigmoid, dens)))

knitr::kable(table)
```

##simulation study


## Shiny App
This package provides a small web application, built with the shiny package.
To use the shiny app call `shiny_kde()`.

## References
1. Estimator selection: a new method with applications to kernel density estimation C. Lacour, P. Massart, V. Rivoirard [2017]: <https://arxiv.org/abs/1607.05091v2>
1. Numerical performance of Penalized Comparison to Overfitting for multivariate kernel density estimation S. Varet, C. Lacour, P. Massart, V. Rivoirard [2019]: <https://arxiv.org/abs/1902.01075>
1. "Nonparametric Estimation" by F. Comte [2017], ISBN: 978-2-36693-030-6 



