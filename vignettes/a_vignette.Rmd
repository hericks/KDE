---
title: "Introduction to KDE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to KDE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
(Erster Satz (vor dem ":") vllt umformulieren?)
The question for which this package tries to give an answer is the following: given $n$ (numeric) samples,
which are distributed according to a cumulative distribution function $F$ and density $f$ (with respect to the Lebesgue measure), how can we estimate the density function $f$?

In this package kernel density estimation is used to solve this problem and our estimator for the desired density function $f$ with samples $x_1,...,x_n$ will be the
function
\begin{align}
\hat{f_h}(x) = \frac{1}{hn} \sum\limits^{n}_{i=0} K(\frac{x_i-x}{h})
\end{align} 
which is called the "kernel density estimator" (KDE).
The KDE depends on a real number $\mathcal{h}$, the so called "bandwidth" and an integrable function $K: \mathbb{R} \rightarrow \mathbb{R}$ such 
that $\int\limits^{\infty}_{-\infty}K(x)dx=1$, which is called "kernel".
The package is equipped with the function `kernel_density_estimator`, which is an implementation of the function described above. 

With the three bandwidth estimation functions: `cross_validation`, `goldenshluger_lepski` and `pco_method` the package focuses 
on solving the problem by estimating the optimal bandwidth for a given kernel.

## Getting started
To load the package with: 
```{r setup}
library(KDE)
```

## Base structure

In order to secure consistency and fulfillment of the requirements for each of the elements used in solving the KDE problem, this package works on S3 classes for densities and kernels.
Both classes inherit from the class `?IntegrableFunction`.
First of all, we need to construct our Objects.

### Density
The density function we want to sample from, is a sine curve on the interval $[0,1]$. If samples are already given, this step will not be necessary.
```{r}
# Custom density
f_dens <- function(x) {
  ret <- 1 + sin(2*pi*x)
  ret[x < 0 | 1 < x] <- 0
  ret
}
support_density <- c(0,1)
# constructing the Density object
dens <- Density(f_dens, support_density, subdivisions=100L)
print(dens)
```
As one can see, the density is a object of class Density. In addition to the function, the object is holding the support and a subdivisions parameter. This is needed for the `stats::integrate` function to work properly.
To see the requirements for constructing a Density object use `?Density`.

### Kernel
This package gives You the possibility to build your own custom kernels:
```{r}
# or build a Kernel object yourself: this is the sigmoid function
f_ker <- function(x){
  return(2/pi * (1/(exp(x) + exp(-x))))
}
support_sigmoid <- c(-20,20)
# constructing the Kernel object
sigmoid_kernel <- Kernel(f_ker, support_sigmoid, subdivisions=10L)
print(sigmoid_kernel)
```
To see the requirements for constructing a Kernel object use `?Kernel`.

### Builtin Kernels
This package also provides a selection of various built-in kernels:
```{r}
# using a builtin Kernel
gaussian_kernel <- gaussian
print(gaussian_kernel)

```
Run `?Kernel` to see the full list of the built-in kernels.

### Sampling 
The KDE package provides a built-in rejection sampling method to create a set of samples from Density objects: 
```{r}
# Create sampler from custom density
g_den <- Density(dunif, c(0,1))
custom_sampler <- rejection_sampling(dens, g_den, runif, 2)
# Sample from custom sampler
num_samples <- 100
samples <- custom_sampler(num_samples)
```
Now that we have choosen our kernel and a set of samples, we have to determine another suitable parameter for a good estimation:

## Bandwidth Estimation

Bandwidth estimation is used to choose a bandwidth $\mathcal{h}$ from a given set $\mathcal{H}_n$ to minimize the maximal risk $\mathbb{E}(||\hat{f_h}-f||^2)$, where $\hat{f_h}$ is the KDE with bandwidth $\mathcal{h}$ and $f$ the density estimated. Hence $f$ is unknown it is not possible to calculate the risk directly. Therefore this package provides three methods for choosing the optimal bandwitdth. 

### Logarithmic Bandwidth Set

The function `logarithmic_bandwidth_set` helps choosing a admissible bandwidth set in a given range.  
```{r}
# setting up bandwidth sets
bandwidth_set <- logarithmic_bandwidth_set(from=1/length(samples), to=1, length.out=10)
```
Note that for the bandwidth estimations `length.out` has to be less than or equal to `length(samples)`.


### Cross Validation
The first method to estimate a bandwidth is called *cross-validation* or *leave-one-out*. 
```{r}
# cross-validation method
cv_gaussian <- cross_validation(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
cv_sigmoid <- cross_validation(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=500L)
cat("bandwidth for gaussian kernel: ", cv_gaussian, "\nbandwidth for sigmoid kernel: ", cv_sigmoid)
```

### Goldenshluger-Lepski Method
The second method is a implementation of the *Goldenshluger-Lepski method*.
```{r}
# goldenshluger-lepski method
gl_gaussian <- goldenshluger_lepski(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
gl_sigmoid <- goldenshluger_lepski(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=200L)
cat("bandwidth for gaussian kernel: ", gl_gaussian, "\nbandwidth for sigmoid kernel: ", gl_sigmoid)
```

### Penalized Comparison to Overfitting
The third approach for estimating a bandwidth is called *Penalized Comparison to Overfitting (PCO)*
```{r}
# Penalized Comparison to Overfitting
pco_gaussian <- pco_method(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
pco_sigmoid <- pco_method(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=250L)
cat("bandwidth for gaussian kernel: ", pco_gaussian, "\nbandwidth for sigmoid kernel: ", pco_sigmoid)
```


## Kernel-Density-Estimation
Now that we have some suggestions for our bandwidth $h$ we can finally estimate our density function $f$ using the kernel density estimator.

### Calculating the KDE function
```{r}
kde_cv_gaussian <- kernel_density_estimator(gaussian_kernel, samples, cv_gaussian, subdivisions=100L)
kde_gl_gaussian <- kernel_density_estimator(gaussian_kernel, samples, gl_gaussian, subdivisions=100L)
kde_pco_gaussian <- kernel_density_estimator(gaussian_kernel, samples, pco_gaussian, subdivisions=100L)

kde_cv_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, cv_sigmoid, subdivisions=200L)
kde_gl_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, gl_sigmoid, subdivisions=200L)
kde_pco_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, pco_sigmoid, subdivisions=200L)
```
Note that the  `kernel_density_estimator` returns a `IntegrableFunction` object. 

## Kernel and bandwidth comparison 
Note that some bandwidth estimators can choose the same bandwidth and therefore, their KDE will be the same. For a little comparison of our calculated estimators, we will use the following ISE function:
```{r}
# comparing the ISE
ISE <- function(kde, dens, subdivisions=100L){
  support <- range(kde$support, dens$support)
  integrate(function(x){(kde$fun(x)-dens$fun(x))^2}, 
                   lower=support[1], upper=support[2], 
                   subdivisions=subdivisions)$value
}
```

```{r,echo=FALSE} 

# ISE for the gaussian kernel
cat("ISE for cross validation on the gaussian kernel: ", ISE(kde_cv_gaussian, dens))
cat("ISE for Goldenshluger-Lepski method on the gaussian kernel: ", ISE(kde_gl_gaussian, dens))
cat("ISE for PCO method on the gaussian kernel: ", ISE(kde_pco_gaussian, dens))

```
```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
# plotting KDE and functionx_lim
x_lim_lower <- -0.5
x_lim_upper<- 1.5
x <- seq(from = x_lim_lower, to = x_lim_upper, length.out=1000)
plot(x, dens$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the gaussian kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2

)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_gaussian$fun(x), col = "violet")
lines(x,
      kde_gl_gaussian$fun(x), col = "steelblue2")
lines(x,
      kde_pco_gaussian$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")




```

```{r,echo=FALSE} 
# ISE for the sigmoid kernel
cat("ISE for cross validation method on the sigmoid kernel: ", ISE(kde_cv_sigmoid, dens))
cat("ISE for Goldenshluger-Lepski method method on the sigmoid kernel: ", ISE(kde_gl_sigmoid, dens))
cat("ISE for PCO method method on the sigmoid kernel: ", ISE(kde_pco_sigmoid, dens))

```

```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
plot(x, dens$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the sigmoid kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2
)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_sigmoid$fun(x), col = "violet")
lines(x,
      kde_gl_sigmoid$fun(x), col = "steelblue2")
lines(x,
      kde_pco_sigmoid$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")

```
At last, a little side by side comparison of the ISE:
```{r, echo=FALSE}
table <- data.frame(method=c("cross_validation ISE", "goldenshluger_lepski ISE", "pco_method ISE"),
                    gaussian=c(ISE(kde_cv_gaussian, dens), ISE(kde_gl_gaussian, dens), ISE(kde_pco_gaussian, dens)),
                    sigmoid=c(ISE(kde_cv_sigmoid, dens), ISE(kde_gl_sigmoid, dens),  ISE(kde_pco_sigmoid, dens)))

knitr::kable(table)
```

## Shiny App
This package provides a small web application, built with the shiny package.
To use the shiny app call `shiny_kde()`.

## References
1. Estimator selection: a new method with applications to kernel density estimation C. Lacour, P. Massart, V. Rivoirard [2017]: <https://arxiv.org/abs/1607.05091v2>
1. Numerical performance of Penalized Comparison to Overfitting for multivariate kernel density estimation S. Varet, C. Lacour, P. Massart, V. Rivoirard [2019]: <https://arxiv.org/abs/1902.01075>
1. "Nonparametric Estimation" by Comte [2017], ISBN: 978-2-36693-030-6 



