---
title: "Introduction to kernel density estimation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to kernel density estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Given realizations $x_1, \dots, x_n$ of $n$ independent and identically distributed (i.i.d.) random variables $X_1, \dots, X_n \sim \mathbb{P}$ with density $f$ w.r.t. the Lebesgue measure $\lambda$, a *kernel density estimator* (KDE) for the density $f$ is defined as

\begin{align*}
  \hat f_h(x) = \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
\end{align*}

The estimator depends on the *kernel* $K: \mathbb{R} \rightarrow \mathbb{R}, K \in \mathscr{L}_1(\mathbb{R}, \lambda)$ with $\lambda(K) = 1$ and the *bandwidth* $h \in \mathbb{R}^+_{\setminus 0}$.

This package aims to provide the necessary tools for practical kernel density estimation and bandwidth estimation. The estimator can be constructed using `kernel_density_estimator`. The bandwidth-selection algorithms are accessible through `cross_validation`, `goldenshluger_lepski` and `pco_method`.

## Getting started
We start off by loading the package

```{r setup, include=FALSE}
knitr::opts_chunk$set(dpi=300,fig.width=7)
set.seed(1)
```

```{r}
library(KDE)
```

## Base structure

The main objects in the context of kernel density estimation are kernels and densities. Since many algorithms rely on the properties of these objects, `KDE` deploys the S3 classes `Density` and `Kernel` (inheriting from `IntegrableFunction`) to numerically ensure some of mathematical conditions (cf. `?IntegrableFunction`). 

These objects require a numerically compact support for the represented function. This may seem like strong restriction at first, but since every integrable function is arbitrary close to zero outside of a compact set, such a support can be found for all integrable functions.

Let's consider these objects in practice. 

### Densities
One can easily construct `Density` objects from the Base-R `stats::dDISTR` functions.
```{r}
Density(dnorm, mean=2)
```
The output may seem complex at first. The underlying function is printed as well as the support (which was found numerically). In particular we'd like to draw attention to the numerically compact support

```{r}
all.equal(dnorm(-10), 0)
```

as well as the *subdivisions*.

The `subdivisions` entry is a core component of the `KDE` package and you'll encounter it many times throughout this vignette, as it specifies the accuracy for many of the implemented functions.

In this example the subdivisions indicate, that integrating `dnorm` using the included `integrate_primitive` with 1000 subdivisions yields a result sufficiently close to 1 to validate `dnorm` as `Density` (cf. `?integrate_primitive`).

Constructing a custom density function could look like this
```{r, error=TRUE}
dens_fun <- function(x) {
  (1 + sin(2*pi*x))*(0 <= x)*(x <= 1)
}

# automatic construction does not work in this case
sin_den <- Density(dens_fun)

# specifying the support by hand or increase the number of subdivisions works
sin_den <- Density(dens_fun, subdivisions = 1500L)
sin_den <- Density(dens_fun, support=c(0,1))
sin_den
```

### Kernels
In a similar manner `KDE` gives you the possibility to build `Kernel` objects

```{r}
# the sigmoid function
sigmoid_function <- function(x) 2/pi * (1/(exp(x) + exp(-x)))
all.equal(sigmoid_function(-20), 0)

# constructing the sigmoid kernel
sigmoid_kernel <- Kernel(sigmoid_function, support=c(-20, 20))
```

For the exact requirements for constructing a `Kernel` object and further information consult `?Kernel`.

### Builtin Kernels
This package also provides a selection of various built-in kernels known from the literature

```{r}
# using a built-in Kernel
gaussian_kernel <- gaussian
print(gaussian_kernel)
```

Run `?Kernel` to see the full list of the built-in kernels.

### Sampling 
In many situations it is favorable to create tests based on simulated data. With `rejection_sampling` the package provides a way to draw samples from custom densities (cf. `?rejection_sampling`). 

```{r}
# create sampler from Density object sin_den
custom_sampler <- rejection_sampling(sin_den, Density(dunif), runif, 2)

# sample from custom sampler
num_samples <- 100
samples <- custom_sampler(num_samples)
```

Now that we have created a kernel and generated a set of samples, it is time to choose a suitable bandwidth to finally build the KDE.

## Bandwidth Estimation

The general idea behind all following bandwidth estimation methods is that the optimal bandwidth $\mathcal{h}$ is the one that minimizes the maximal risk 
\begin{align}
\mathbb{E}(||\hat{f_h}-f||^2)
\end{align} where $\hat{f_h}$ is the KDE with bandwidth $\mathcal{h}$ and $f$ the density we like to estimate. Note that to realize this idea, each method chooses the optimal bandwidth from a given finite collection $\mathcal{H}_n$. \
Now the problem lies in the fact that $f$ is unknown, so we can't calculate the risk directly, therefore this package provides three different approaches to solve that and to estimate the most suitable bandwidth from a given collection $\mathcal{H}_n$.

### Logarithmic Bandwidth Set

In case you don't already have a bandwidth collection to work with, you can use the function `logarithmic_bandwidth_set` which provides you with an admissible bandwidth set in a given range.  
```{r}
# setting up bandwidth sets
bandwidth_set <- logarithmic_bandwidth_set(from=1/length(samples), to=1, length.out=10)
```
Note that for the bandwidth estimation methods `length.out` has to be less than or equal to `length(samples)`.


### Cross Validation
The first method to estimate a bandwidth is called *cross-validation* or *leave-one-out*. For more information run `?cross_validation`.
```{r}
# cross validation method
cv_gaussian <- cross_validation(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
cv_sigmoid <- cross_validation(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=500L)
cat("bandwidth for gaussian kernel: ", cv_gaussian, "\nbandwidth for sigmoid kernel: ", cv_sigmoid)
```

### Goldenshluger-Lepski Method
The second approach is a implementation of the *Goldenshluger-Lepski method*. To understand the function in detail see `?goldenshluger_lepski`.
```{r}
# goldenshluger-lepski method
gl_gaussian <- goldenshluger_lepski(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
gl_sigmoid <- goldenshluger_lepski(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=200L)
cat("bandwidth for gaussian kernel: ", gl_gaussian, "\nbandwidth for sigmoid kernel: ", gl_sigmoid)
```

### Penalized Comparison to Overfitting
The third method for estimating a bandwidth is called *Penalized Comparison to Overfitting (PCO)*. If you want to look up further details about the function `pco_method` run `?pco_method`.
```{r}
# Penalized Comparison to Overfitting
pco_gaussian <- pco_method(gaussian_kernel, samples, bandwidths=bandwidth_set, subdivisions=100L)
pco_sigmoid <- pco_method(sigmoid_kernel, samples, bandwidths=bandwidth_set, subdivisions=250L)
cat("bandwidth for gaussian kernel: ", pco_gaussian, "\nbandwidth for sigmoid kernel: ", pco_sigmoid)
```


## Kernel-Density-Estimation
Now that we have determined some suggestions for our bandwidth $h$ we can finally estimate our density function $f$ using the kernel density estimator.

### Calculating the KDE function
```{r}
kde_cv_gaussian <- kernel_density_estimator(gaussian_kernel, samples, cv_gaussian, subdivisions=100L)
kde_gl_gaussian <- kernel_density_estimator(gaussian_kernel, samples, gl_gaussian, subdivisions=100L)
kde_pco_gaussian <- kernel_density_estimator(gaussian_kernel, samples, pco_gaussian, subdivisions=100L)

kde_cv_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, cv_sigmoid, subdivisions=200L)
kde_gl_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, gl_sigmoid, subdivisions=200L)
kde_pco_sigmoid <- kernel_density_estimator(sigmoid_kernel, samples, pco_sigmoid, subdivisions=200L)
```
Note that the  `kernel_density_estimator` returns a `IntegrableFunction` object. 

## Kernel and bandwidth comparison 
Keep in mind that two different bandwidth estimators can choose the same bandwidth and therefore, their KDE will be the same. For a small comparison of our calculated estimators, we will use the following ISE function:
```{r}
# comparing the ISE
ISE <- function(kde, den, subdivisions=100L){
  support <- range(kde$support, den$support)
  integrate(function(x){(kde$fun(x)-den$fun(x))^2}, 
                   lower=support[1], upper=support[2], 
                   subdivisions=subdivisions)$value
}
```

```{r,echo=FALSE} 
# ISE for the gaussian kernel
cat("ISE for cross validation on the gaussian kernel: ", ISE(kde_cv_gaussian, sin_den))

cat("ISE for Goldenshluger-Lepski method on the gaussian kernel: ", ISE(kde_gl_gaussian, sin_den))
cat("ISE for PCO method on the gaussian kernel: ", ISE(kde_pco_gaussian, sin_den))

```
```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
# plotting KDE and functionx_lim
x_lim_lower <- -0.5
x_lim_upper<- 1.5
x <- seq(from = x_lim_lower, to = x_lim_upper, length.out=1000)
plot(x, sin_den$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the gaussian kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2

)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_gaussian$fun(x), col = "violet")
lines(x,
      kde_gl_gaussian$fun(x), col = "steelblue2")
lines(x,
      kde_pco_gaussian$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")
```

```{r,echo=FALSE} 
# ISE for the sigmoid kernel
cat("ISE for cross validation method on the sigmoid kernel: ", ISE(kde_cv_sigmoid, sin_den))
cat("ISE for Goldenshluger-Lepski method method on the sigmoid kernel: ", ISE(kde_gl_sigmoid, sin_den))
cat("ISE for PCO method method on the sigmoid kernel: ", ISE(kde_pco_sigmoid, sin_den))

```

```{r, echo=FALSE, out.width='100%', fig.width=8, fig.height=4, fig.align = "center"}
plot(x, sin_den$fun(x),
     xlim = c(x_lim_lower, x_lim_upper),
     ylim = c(-0.5, 2.5),
     main = "KDE using the sigmoid kernel",
     xlab = "",
     ylab = "",
     col = "dark red",
     type = "l",
     lwd = 2
)
legend("topright", legend = c("density", "samples", "PCO method", "Crossvalidation", "Goldenshluger-Lepski"), col = c("dark red","royal blue", "dark green","violet", "steelblue2"), lty = c(1,1,1,1,1), lwd = c(2,1,1,1,1), cex = 0.75)
lines(x,
      kde_cv_sigmoid$fun(x), col = "violet")
lines(x,
      kde_gl_sigmoid$fun(x), col = "steelblue2")
lines(x,
      kde_pco_sigmoid$fun(x), col = "dark green")
points(samples,
       integer(length(samples)),
       pch = ".",
       col = "blue")

```

At last, a side by side comparison of the above calculated ISE's:
```{r, echo=FALSE}
table <- data.frame(method=c("cross_validation ISE", "goldenshluger_lepski ISE", "pco_method ISE"),
                    gaussian=c(ISE(kde_cv_gaussian, sin_den), ISE(kde_gl_gaussian, sin_den), ISE(kde_pco_gaussian, sin_den)),
                    sigmoid=c(ISE(kde_cv_sigmoid, sin_den), ISE(kde_gl_sigmoid, sin_den),  ISE(kde_pco_sigmoid, sin_den)))

knitr::kable(table)
```

##simulation study


## Shiny App
This package provides a small web application, built with the shiny package.
To use the shiny app call `shiny_kde()`.

## References
1. Estimator selection: a new method with applications to kernel density estimation C. Lacour, P. Massart, V. Rivoirard [2017]: <https://arxiv.org/abs/1607.05091v2>
1. Numerical performance of Penalized Comparison to Overfitting for multivariate kernel density estimation S. Varet, C. Lacour, P. Massart, V. Rivoirard [2019]: <https://arxiv.org/abs/1902.01075>
1. "Nonparametric Estimation" by F. Comte [2017], ISBN: 978-2-36693-030-6 



